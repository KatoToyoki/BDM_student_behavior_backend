# Spark Configuration

# === Application Configuration ===
spark.app.name                          PySpark-Cluster-App
spark.default.parallelism               16
spark.sql.shuffle.partitions            32

# === Memory Management ===
spark.driver.memory                     2g
spark.driver.maxResultSize              1g
spark.executor.memory                   3g
spark.memory.fraction                   0.6
spark.memory.storageFraction            0.5

# === Performance Optimization ===
# Adaptive Query Execution (AQE) - Handles large datasets efficiently
spark.sql.adaptive.enabled              true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.localShuffleReader.enabled true
spark.sql.adaptive.skewJoin.enabled     true

# Dynamic allocation for resource efficiency
spark.dynamicAllocation.enabled         true
spark.dynamicAllocation.minExecutors    2
spark.dynamicAllocation.maxExecutors    5

# === File Format Optimizations ===
# Columnar storage optimizations
spark.sql.parquet.enableVectorizedReader true
spark.sql.parquet.columnarReaderBatchSize 4096

# CSV reading optimizations
spark.sql.csv.parser.columnPruning.enabled true

# === Serialization ===
spark.serializer                        org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer              64k
spark.kryoserializer.buffer.max          64m

# === Network Configuration ===
spark.network.timeout                   800s
spark.sql.broadcastTimeout              36000
spark.storage.blockManagerSlaveTimeoutMs 120s

# === Garbage Collection ===
spark.executor.extraJavaOptions         -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -verbose:gc -Xloggc:/opt/spark/logs/gc.log
spark.driver.extraJavaOptions           -XX:+UseG1GC -XX:+UseStringDeduplication

# === Shuffle Optimization ===
spark.shuffle.service.enabled           true
spark.shuffle.reduceLocality.enabled    false
spark.shuffle.spill.compress             true
spark.shuffle.compress                   true

# === SQL Configuration ===
spark.sql.execution.arrow.pyspark.enabled true
spark.sql.execution.arrow.maxRecordsPerBatch 10000

# === Logging Configuration ===
spark.eventLog.enabled                  true
spark.eventLog.dir                      /opt/spark/logs/spark-events
spark.history.fs.logDirectory          /opt/spark/logs/spark-events

# === UI Configuration ===
spark.ui.reverseProxy                   true
spark.ui.reverseProxyUrl                http://localhost:8080

# === Security (Basic) ===
spark.ui.view.acls.enable               false
spark.modify.acls.enable                false

# === Large Dataset Handling ===
spark.sql.files.openCostInBytes        4194304   # 4MB
spark.sql.files.maxPartitionBytes      134217728 # 128MB per partition
spark.sql.adaptive.advisoryPartitionSizeInBytes 67108864  # 64MB advisory size

# === Data Locality ===
spark.locality.wait                     3s
spark.locality.wait.node                1s
spark.locality.wait.process             1s